{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text_Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:43:10.740590Z",
     "start_time": "2018-03-20T14:43:10.725858Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "# Tokenize the Input Sentences\n",
    "# def tokenize(sentence):\n",
    "#\treturn [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "def tokenize(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "# Vectorize the text\n",
    "# Convert Subtext, Questions to Vector Form\n",
    "def vectorize_ques(data, word_id, test_max_length, ques_max_length):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    for subtext, question in data:\n",
    "        x = [word_id[w] for w in subtext]\n",
    "        xq = [word_id[w] for w in question]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "    return (pad_sequences(X, maxlen=test_max_length),\n",
    "            pad_sequences(Xq, maxlen=ques_max_length))\n",
    "\n",
    "# Vectorize the text\n",
    "# Convert Subtext, Questions, Answers to Vector Form\n",
    "# Y: array[] of zero's with \"1\" corresponding to word representing correct answer\n",
    "def vectorize_text(data, word_id, text_max_length, ques_max_length):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for subtext, question, answer in data:\n",
    "        x = [word_id[w] for w in subtext]\n",
    "        # Save the ID of Questions using SubText\n",
    "        xq = [word_id[w] for w in question]\n",
    "        # Save the answers for the Questions in \"Y\" as \"1\"\n",
    "        y = np.zeros(len(word_id) + 1)\n",
    "        y[word_id[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=text_max_length),\n",
    "            pad_sequences(Xq, maxlen=ques_max_length),\n",
    "            np.array(Y))\n",
    "\n",
    "# Read the text files\n",
    "def read_text():\n",
    "    text = []\n",
    "    input_line = input('Story, Empty to stop: ')\n",
    "    while input_line != '':\n",
    "        # for now, lines have to be a full sentence\n",
    "        if not input_line.endswith('.'):\n",
    "            input_line += '.'\n",
    "        text.extend(tokenize(input_line))\n",
    "        input_line = input('Story, Empty to stop: ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T14:44:49.280484Z",
     "start_time": "2018-03-20T14:44:49.215440Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset from Facebook AI Research Page\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tarfile\n",
    "from Text_Preprocessing import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Parse the text data from bAbI tasks format.\n",
    "# If only_supporting is True, only the sentences supporting the answer are kept\n",
    "# Data format: id Text\n",
    "def parse_text(lines, only_supporting=False):\n",
    "    # Make two new Lists/Arrays to store data and text\n",
    "    data = []\n",
    "    text = []\n",
    "    # Read the text from bAbI Dataset\n",
    "    for line in lines:\n",
    "        # Lines input from Text\n",
    "        # Format: ID Line\n",
    "        line = line.decode('utf-8').strip()\n",
    "        # Separate ID and Text from Input Lines\n",
    "        # Lines contain both Text as well as Question Answers\n",
    "        id, line = line.split(' ', 1)\n",
    "        # Convert ID to int type\n",
    "        id = int(id)\n",
    "        # If ID = 1, it is the text/story\n",
    "        if id == 1:\n",
    "            text = []\n",
    "        # If there is a tab space in the input lines, it contains Question, Answer, Supporting Text ID\n",
    "        # and the Supporting Line Number in the Text\n",
    "        # Format: Question? Answer Line_Number\n",
    "        if '\\t' in line:\n",
    "            ques, ans, supporting = line.split('\\t')\n",
    "            # Take in the Question and Tokenize it into words\n",
    "            ques = tokenize(ques)\n",
    "            subtext = None\n",
    "            # Keep only the supporting text from Question; only_supporting = True\n",
    "            if only_supporting:\n",
    "                # Map the Supporting Text ID as int\n",
    "                supporting = list(map(int, supporting.split()))\n",
    "                # subtext: List of the sentences supporting the Questions\n",
    "                subtext = [text[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Contains all the related Text Lines in the file\n",
    "                # Relation using Supporting ID\n",
    "                subtext = [x for x in text if x]\n",
    "\n",
    "            # Data containes tokenized first two sentences, then the answers.\n",
    "            # All tokenized words in form of arrays\n",
    "            # Tokenized text in form of array of array\n",
    "            # All this in a List\n",
    "            # data: array of all such Lists\n",
    "            # Format: [([[First sentence Tokeinized],[Second Sentence Tokenized]],[Question with Answer Tokenized]), ....]\n",
    "            data.append((subtext, ques, ans))\n",
    "            text.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            text.append(sent)\n",
    "    return data\n",
    "\n",
    "# Read the file, retrieve the stories and convert sentences into a single story\n",
    "def get_stories(file, only_supporting=False, max_length=None):\n",
    "    # Data containes tokenized first two sentences, then the answers.\n",
    "    # All tokenized words in form of arrays\n",
    "    # Tokenized text in form of array of array\n",
    "    # All this in a List\n",
    "    # data: array of all such Lists\n",
    "    # Format: [([[First sentence Tokeinized],[Second Sentence Tokenized]],[Question with Answer Tokenized]), ....]\n",
    "    data = parse_text(file.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    # flatten: Takes two sentences and makes one array, 2nd array of Question answer in a list\n",
    "    # Format: [([First sentence Tokeinized, Second Sentence Tokenized],[Question with Answer Tokenized]), ....]\n",
    "    data = [(flatten(text), question, answer) for text, question, answer in data if\n",
    "            not max_length or len(flatten(text)) < max_length]\n",
    "    return data\n",
    "\n",
    "class memoryNetwork(object):\n",
    "    FILE_NAME = 'model'\n",
    "    VOCAB_FILE_NAME = 'model_vocab'\n",
    "    def __init__(self):\n",
    "        if (os.path.exists(memoryNetwork.FILE_NAME) and\n",
    "                os.path.exists(memoryNetwork.VOCAB_FILE_NAME)):\n",
    "            self.load()\n",
    "        else:\n",
    "            self.train()\n",
    "            self.store()\n",
    "\n",
    "    def load(self):\n",
    "        self.model = keras.models.load_model(memoryNetwork.FILE_NAME)\n",
    "        with open(memoryNetwork.VOCAB_FILE_NAME, 'rb') as file:\n",
    "            self.word_id = pickle.load(file)\n",
    "    def store(self):\n",
    "        self.model.save(memoryNetwork.FILE_NAME)\n",
    "        with open(memoryNetwork.VOCAB_FILE_NAME, 'wb') as file:\n",
    "            pickle.dump(self.word_id, file)\n",
    "\n",
    "    def train(self):\n",
    "        # Load the bAbI Dataset\n",
    "        try:\n",
    "            dataPath = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                                origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "        except:\n",
    "            print('Error downloading dataset, please download it manually:\\n'\n",
    "                  '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "                  '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "            raise\n",
    "\n",
    "        tar = tarfile.open(dataPath)\n",
    "\n",
    "        # Load the Single Supporting Fact and Two Supporting Fact files\n",
    "        challenges = {\n",
    "            # QA1 with 10,000 samples\n",
    "            'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "            # QA2 with 10,000 samples\n",
    "            'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "        }\n",
    "\n",
    "        challenge_type = 'single_supporting_fact_10k'\n",
    "\n",
    "        challenge = challenges[challenge_type]\n",
    "\n",
    "        # Extract the Text from single_supporting_fact_10k file\n",
    "        print('Extracting stories for the challenge:', challenge_type)\n",
    "\n",
    "        # Load the Testing and Training Text Data\n",
    "        train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "        test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "        # Initialize vocabulary as as Set\n",
    "        # Create a Vocabulary list with all words occuring only once\n",
    "        vocab = set()\n",
    "        for text, ques, answer in train_stories + test_stories:\n",
    "            vocab |= set(text + ques + [answer])\n",
    "\n",
    "        # Sort the words in Vocabulary List\n",
    "        vocab = sorted(vocab)\n",
    "\n",
    "        # Get the max length of the Vocabulary, text and Questions\n",
    "        vocab_size = len(vocab) + 1\n",
    "\n",
    "        # text_max_length: length of th subtext; no. of subtexts\n",
    "        text_max_length = max(list(map(len, (x for x, _, _ in train_stories + test_stories))))\n",
    "\n",
    "        # ques_max_length: length of questions in input.\n",
    "        ques_max_length = max(list(map(len, (x for _, x, _ in train_stories + test_stories))))\n",
    "\n",
    "        print('-')\n",
    "        print('Vocab size:', vocab_size, 'unique words')\n",
    "        print('Story max length:', text_max_length, 'words')\n",
    "        print('Query max length:', ques_max_length, 'words')\n",
    "        print('Number of training stories:', len(train_stories))\n",
    "        print('Number of test stories:', len(test_stories))\n",
    "        print('-')\n",
    "        print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "        print(train_stories[0])\n",
    "        print('-')\n",
    "        print('Vectorizing the word sequences...')\n",
    "        # Vectorize the Training and Testing Data\n",
    "        self.word_id = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "        # inputs_train: Matrix of Arrays; Arrays containing vectorized sentences\n",
    "        # ques_train: Matrix of Arrays; Each array has 4 values; Each value corresponds to a character.\n",
    "        # answers_train: Matrix of Arrays; Each array contains a single \"1\", index corresponding to answer\n",
    "        inputs_train, ques_train, answers_train = vectorize_text(train_stories,\n",
    "                                                                 self.word_id,\n",
    "                                                                 text_max_length,\n",
    "                                                                 ques_max_length)\n",
    "\n",
    "        inputs_test, ques_test, answers_test = vectorize_text(test_stories,\n",
    "                                                              self.word_id,\n",
    "                                                              text_max_length,\n",
    "                                                              ques_max_length)\n",
    "\n",
    "\n",
    "\n",
    "        # Dataset Analysis\n",
    "        print('-')\n",
    "        print('inputs: integer tensor of shape (samples, max_length)')\n",
    "        print('inputs_train shape:', inputs_train.shape)\n",
    "        print('inputs_test shape:', inputs_test.shape)\n",
    "        print('-')\n",
    "        print('queries: integer tensor of shape (samples, max_length)')\n",
    "        print('queries_train shape:', ques_train.shape)\n",
    "        print('queries_test shape:', ques_test.shape)\n",
    "        print('-')\n",
    "        print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "        print('answers_train shape:', answers_train.shape)\n",
    "        print('answers_test shape:', answers_test.shape)\n",
    "        print('-')\n",
    "        print('Compiling...')\n",
    "        # Define Placeholders\n",
    "        input_sequence = Input((text_max_length,))\n",
    "        question = Input((ques_max_length,))\n",
    "        # ---------------------------------- Encode the Data ----------------------------------------\n",
    "        # Embed the input sequence into a sequence of vectors\n",
    "        input_encoder_m = Sequential()\n",
    "        input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                                      output_dim=64))\n",
    "        input_encoder_m.add(Dropout(0.3))\n",
    "        # Output: (samples, text_maxlen, embedding_dim)\n",
    "        # Embed the input into a sequence of vectors of size ques_max_length\n",
    "        input_encoder_c = Sequential()\n",
    "        input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                                      output_dim=ques_max_length))\n",
    "        input_encoder_c.add(Dropout(0.3))\n",
    "        # output: (samples, story_maxlen, query_maxlen)\n",
    "        # Embed the question into a sequence of vectors\n",
    "        question_encoder = Sequential()\n",
    "        question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                                       output_dim=64,\n",
    "                                       input_length=ques_max_length))\n",
    "        question_encoder.add(Dropout(0.3))\n",
    "        # output: (samples, query_maxlen, embedding_dim)\n",
    "        # Encode input sequence and questions (which are indices)\n",
    "        # to sequences of dense vectors\n",
    "        input_encoded_m = input_encoder_m(input_sequence)\n",
    "        input_encoded_c = input_encoder_c(input_sequence)\n",
    "        question_encoded = question_encoder(question)\n",
    "\n",
    "        # compute a 'match' between the first input vector sequence\n",
    "        # and the question vector sequence\n",
    "        # shape: `(samples, story_maxlen, query_maxlen)`\n",
    "        match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "        match = Activation('softmax')(match)\n",
    "\n",
    "        # add the match matrix with the second input vector sequence\n",
    "        response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "        response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "        # concatenate the match matrix with the question vector sequence\n",
    "        answer = concatenate([response, question_encoded])\n",
    "\n",
    "        # the original paper uses a matrix multiplication for this reduction step.\n",
    "        # we choose to use a RNN instead.\n",
    "        answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "        # one regularization layer -- more would probably be needed.\n",
    "        answer = Dropout(0.3)(answer)\n",
    "        answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "        # we output a probability distribution over the vocabulary\n",
    "        answer = Activation('softmax')(answer)\n",
    "\n",
    "        # build the final model\n",
    "        self.model = Model([input_sequence, question], answer)\n",
    "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train the Model\n",
    "        self.model.fit([inputs_train, ques_train], answers_train,\n",
    "                  batch_size=32,\n",
    "                  epochs=120,\n",
    "                  validation_data=([inputs_test, ques_test], answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input Story and Get Answer as output using Trained Model.\n",
    "import numpy as np\n",
    "memory_network = memoryNetwork()\n",
    "while True:\n",
    "    print('Use this Vocabulary to form Questions:\\n' + ' , '.join(memory_network.word_id.keys()))\n",
    "    story = read_text()\n",
    "    print('Story: ' + ' '.join(story))\n",
    "    question = input('q:')\n",
    "    if question == '' or question == 'exit':\n",
    "        break\n",
    "    story_vector, query_vector = vectorize_ques([(story, tokenize(question))],\n",
    "                                                  memory_network.word_id, 68, 4)\n",
    "    prediction = memory_network.model.predict([np.array(story_vector), np.array(query_vector)])\n",
    "    prediction_word_index = np.argmax(prediction)\n",
    "    for word, index in memory_network.word_id.items():\n",
    "        if index == prediction_word_index:\n",
    "            print('Answer: ',word)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
